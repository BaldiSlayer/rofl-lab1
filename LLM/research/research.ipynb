{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6p9cJFLdEfS1"
   },
   "source": [
    "### Гипотезы\n",
    "1. Убрать ненужные слова по типу: расскажи, докажи, из, ...\n",
    "\n",
    "    Есть два пути: выбрать опорные слова из определений и оставить только их или собрать служебные и второстепенные слова и выкинуть служебные\n",
    "2. На каком языке лучше (хотим русский)\n",
    "3. Дообучить русскую нейронку (недавно вышла Т-pro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBdqyMMU0_GZ"
   },
   "source": [
    "## качаю нужные библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8nxzUr8-3NsH",
    "outputId": "0438dc07-03b1-46d9-be9d-e352563190d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymorphy2\n",
      "  Downloading pymorphy2-0.9.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dawg-python>=0.7.1 (from pymorphy2)\n",
      "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n",
      "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting docopt>=0.6 (from pymorphy2)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
      "Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=f4c8a345c1b227573ae4c4d2cb8c923154a517be4a90d1fde17221369e36d7a1\n",
      "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
      "Successfully built docopt\n",
      "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
      "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "E50FJEIezntI",
    "outputId": "08a7a992-129e-417e-db52-74076effc814"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfminer.six\n",
      "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.4.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (43.0.3)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
      "Downloading pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/5.6 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/5.6 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pdfminer.six\n",
      "Successfully installed pdfminer.six-20240706\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "SSsLEe_zznq9",
    "outputId": "a56346db-5d2b-497b-8e54-eab5466a41d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfminer.six[image] in /usr/local/lib/python3.10/dist-packages (20240706)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six[image]) (3.4.0)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six[image]) (43.0.3)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from pdfminer.six[image]) (11.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six[image]) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six[image]) (2.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install 'pdfminer.six[image]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyaR7pFa1Jij"
   },
   "source": [
    "## всякие импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PMtjmNHRSAE3"
   },
   "outputs": [],
   "source": [
    "import re # регулярки\n",
    "import nltk # для стоп-слов\n",
    "from nltk.tokenize import sent_tokenize # для токенизации\n",
    "from nltk.tokenize import RegexpTokenizer # разбиваем на слова\n",
    "from nltk.corpus import stopwords # убираем стоп слова\n",
    "import pymorphy2 # для лемантизации\n",
    "from collections import Counter # подсчёт частых\n",
    "from gensim.models.phrases import Phrases, Phraser # дли биграмм\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "from pdfminer.high_level import extract_text # pdf в текст\n",
    "\n",
    "from google.colab import drive # подключение к диску"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pe5CpIvY0RY7",
    "outputId": "becca389-1a1a-4fee-f6be-7f6c51260b8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1jjMPO7aSlRS",
    "outputId": "8efedf04-17d8-49fd-8951-1cea2baa1e4c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "id": "dQ6xQf3A3dSf"
   },
   "outputs": [],
   "source": [
    "stopwords_ru = stopwords.words('russian')\n",
    "tokenizer = RegexpTokenizer('\\w+')\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-KjpkXL2vZh"
   },
   "source": [
    "## начинаем работу с текстами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ot4HjDfUDErj"
   },
   "source": [
    "### книга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "VvKlY3SokHGK"
   },
   "outputs": [],
   "source": [
    "# лекции за 2023 год + материалы итмо + книга хопкрофта на русском\n",
    "all_texts = []\n",
    "\n",
    "for i in range(1, 68):\n",
    "    t = extract_text(f\"/content/drive/MyDrive/texts/{i}.pdf\")\n",
    "    # глазами зацепилась за не оч хорошие переносы, поэтому фикс\n",
    "    t = t.replace('-\\n', '')\n",
    "    all_texts.append(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iuzfpEoDHi9"
   },
   "source": [
    "## код"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxCQYjIPsd65"
   },
   "source": [
    "### какие-то мои функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "rg8_EDpD8yhb"
   },
   "outputs": [],
   "source": [
    "# подсчёт метрики k\n",
    "def my_metrics(user, db, clean_dict=None, trash_dict=None):\n",
    "    user = make_clean(user)\n",
    "    db = make_clean(db)\n",
    "\n",
    "    user = get_words_only(user)\n",
    "    db = get_words_only(db)\n",
    "\n",
    "    if clean_dict is not None:\n",
    "        user = [w for w in user if w in my_dict]\n",
    "        db = [w for w in db if w in my_dict]\n",
    "\n",
    "    if trash_dict is not None:\n",
    "        user = [w for w in user if w not in trash_dict]\n",
    "        db = [w for w in db if w not in trash_dict]\n",
    "\n",
    "    print(user)\n",
    "    print(db)\n",
    "\n",
    "    in_both = list(set(user) & set(db))\n",
    "\n",
    "    return len(in_both) / (len(db) + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "LhYu7mWu9wSF"
   },
   "outputs": [],
   "source": [
    "# берём только слова из предложений\n",
    "def get_words_only(sentences_tokenize):\n",
    "    words = [item for sent in sentences_tokenize for item in sent]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "RFvD0NeL2Wvu"
   },
   "outputs": [],
   "source": [
    "def make_clean(text):\n",
    "    # чистим текст\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\n|\\t|\\r', ' ', text)\n",
    "    # преобразуем в предложения\n",
    "    sentences = sent_tokenize(text)\n",
    "    # бъём предложения на слова\n",
    "    sentences_tokenize = [tokenizer.tokenize(item) for item in sentences]\n",
    "    # убираем стоп слова\n",
    "    sentences_tokenize = [[item for item in sent if (item not in stopwords_ru and re.match('^[а-я]*$', item))] for sent in sentences_tokenize]\n",
    "    # лемантизируем слова\n",
    "    sentences_tokenize = [[morph.normal_forms(item)[0] for item in sent] for sent in sentences_tokenize]\n",
    "\n",
    "    return sentences_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "QCoWUMKk30Kf"
   },
   "outputs": [],
   "source": [
    "# словарь популярных слов\n",
    "def get_dict_words(sentences_tokenize):\n",
    "    words = [item for sent in  sentences_tokenize for item in sent]\n",
    "    word_dict = Counter(words)\n",
    "    common = word_dict.most_common()\n",
    "\n",
    "    return common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "rtzmzqEu8iQh"
   },
   "outputs": [],
   "source": [
    "# полезные слова = часто встречаем их\n",
    "def get_useful(common):\n",
    "    my_dict = {}\n",
    "\n",
    "    for i, j in common:\n",
    "        if i != 'ε' and len(i) > 2 and re.match('^[а-я]*$', i):\n",
    "            my_dict[i] = j\n",
    "\n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLyXh4KBsh7A"
   },
   "source": [
    "### смотрим на текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "id": "tnfIQEDcpTXn"
   },
   "outputs": [],
   "source": [
    "epsilon = 0.001\n",
    "\n",
    "all_sentences = []\n",
    "all_words = []\n",
    "\n",
    "my_dict = {}\n",
    "\n",
    "# мусорные конструкции\n",
    "trash_dict = ['че', 'доказать', 'рассказать', 'ваш', 'привести', 'сделать', 'описать', 'дать', 'что']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "id": "ntRI3jSxo-Xi"
   },
   "outputs": [],
   "source": [
    "for text in all_texts:\n",
    "    t = make_clean(text)\n",
    "    all_sentences = all_sentences + t\n",
    "\n",
    "    # частые слова\n",
    "    common = get_dict_words(t)\n",
    "    all_words += [i for (i, j) in common]\n",
    "    # «полезные» слова\n",
    "    useful_dict = get_useful(common)\n",
    "\n",
    "    new_dict = dict(list(my_dict.items()) + list(useful_dict.items()))\n",
    "    my_dict = new_dict.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DqvEnLZ9spVQ"
   },
   "source": [
    "### предобработка перед w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "id": "jSfuYLAAstDw"
   },
   "outputs": [],
   "source": [
    "bigram = Phrases(all_sentences)\n",
    "bigram_transformer = Phraser(bigram)\n",
    "\n",
    "# генератор текстов с биграммами\n",
    "def bigram_generator():\n",
    "    for text in all_sentences:\n",
    "        yield bigram_transformer[[word for word in text]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "id": "SLWhw_oRvJUp"
   },
   "outputs": [],
   "source": [
    "trigram = Phrases(bigram_generator())\n",
    "trigram_transformer = Phraser(trigram)\n",
    "\n",
    "def trigram_generator():\n",
    "    for text in all_sentences:\n",
    "        yield trigram_transformer[bigram_transformer[[word for word in text]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true,
    "id": "_bXigcMdvQLb"
   },
   "outputs": [],
   "source": [
    "words = [i for i in trigram_generator()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "id": "fU_fJ8wnvXDt"
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(vector_size=300, window=7, min_count=3)\n",
    "model.build_vocab(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SOR_qrPIvdNd",
    "outputId": "4fd22705-115d-4231-971d-2b7460008482"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211850, 268615)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(words, total_examples=model.corpus_count, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N1Up3Fc31jjO",
    "outputId": "6cc8392b-cf95-4889-b80a-cf68b1b0819b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999796"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('слово', 'цепочка')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2QHea_0vgBl"
   },
   "outputs": [],
   "source": [
    "# сохраню потом\n",
    "model.save('wv_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "irA_1fUlviOa"
   },
   "outputs": [],
   "source": [
    "trigram_transformer.save('wv_trigramm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hc3EmqlPrQjB"
   },
   "source": [
    "# подвал"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "yvs_UWnh2WsV"
   },
   "outputs": [],
   "source": [
    "# чистим текст из пдфки\n",
    "test = make_clean(all_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "id": "mZhO3Buy4Sis"
   },
   "outputs": [],
   "source": [
    "# словарь с частыми словами\n",
    "common = get_dict_words(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "ubMP3X-2-ixl"
   },
   "outputs": [],
   "source": [
    "# словарь с полезными терминами\n",
    "my_dict = get_useful(common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "jW752ahHdmU0"
   },
   "outputs": [],
   "source": [
    "s1 = 'че такое эти ваши производные антимирова'\n",
    "\n",
    "s2 = 'Как можно вычислить частичные производные Антимирова?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pgytyVDsd6LS",
    "outputId": "7f320050-d35a-4d57-e808-2e3489606526"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['такой', 'производный', 'антимиров']\n",
      "['вычислить', 'частичный', 'производный', 'антимиров']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4998750312421894"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_metrics(s1, s2, trash_dict=trash_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6QZNMrgDqvw"
   },
   "source": [
    "и тут для меня дошло, что, если расписывать вопросы в БЗ, то деление на длину слов вопроса из БЗ (коих будет много) не будет явно и точно отражать близость, но относительно такой штуки можно смотреть динамику"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "eBdqyMMU0_GZ",
    "Ot4HjDfUDErj",
    "Hc3EmqlPrQjB"
   ],
   "provenance": []
  },
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
