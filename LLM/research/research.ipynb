{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p9cJFLdEfS1"
      },
      "source": [
        "### Гипотезы\n",
        "1. Убрать ненужные слова по типу: расскажи, докажи, из, ...\n",
        "\n",
        "    Есть два пути: выбрать опорные слова из определений и оставить только их или собрать служебные и второстепенные слова и выкинуть служебные\n",
        "2. На каком языке лучше (хотим русский)\n",
        "3. Дообучить русскую нейронку (недавно вышла Т-pro)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBdqyMMU0_GZ"
      },
      "source": [
        "## качаю нужные библиотеки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nxzUr8-3NsH",
        "outputId": "3695f324-f2b5-4762-de99-a9abf732445e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting dawg-python>=0.7.1 (from pymorphy2)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting docopt>=0.6 (from pymorphy2)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=0fd2689e7a8f948320105d2df5b9717396c4afb011a9b61614a9e02f743d381e\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ],
      "source": [
        "!pip install pymorphy2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "E50FJEIezntI",
        "outputId": "303d3cf4-aa08-43da-8ae6-3c5cbd4ad45d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Downloading pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20240706\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfminer.six"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SSsLEe_zznq9",
        "outputId": "da9f2505-0d7d-4a1b-8f59-2f519b076990"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pdfminer.six[image] in /usr/local/lib/python3.10/dist-packages (20240706)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six[image]) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six[image]) (43.0.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from pdfminer.six[image]) (11.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six[image]) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six[image]) (2.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install 'pdfminer.six[image]'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyaR7pFa1Jij"
      },
      "source": [
        "## всякие импорты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMtjmNHRSAE3"
      },
      "outputs": [],
      "source": [
        "import re # регулярки\n",
        "import nltk # для стоп-слов\n",
        "from nltk.tokenize import sent_tokenize # для токенизации\n",
        "from nltk.tokenize import RegexpTokenizer # разбиваем на слова\n",
        "from nltk.corpus import stopwords # убираем стоп слова\n",
        "import pymorphy2 # для лемантизации\n",
        "from collections import Counter # подсчёт частых\n",
        "from gensim.models.phrases import Phrases, Phraser # дли биграмм\n",
        "from scipy import spatial\n",
        "\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "from pdfminer.high_level import extract_text # pdf в текст\n",
        "\n",
        "from google.colab import drive # подключение к диску"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pe5CpIvY0RY7",
        "outputId": "ba38775a-d3bb-4fc6-dd12-2ab9f2eacace"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jjMPO7aSlRS",
        "outputId": "92d8c29f-0c5c-4069-fcb0-e15ccc8a7752"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dQ6xQf3A3dSf"
      },
      "outputs": [],
      "source": [
        "stopwords_ru = stopwords.words('russian')\n",
        "tokenizer = RegexpTokenizer('\\w+')\n",
        "morph = pymorphy2.MorphAnalyzer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-KjpkXL2vZh"
      },
      "source": [
        "## начинаем работу с текстами"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot4HjDfUDErj"
      },
      "source": [
        "### книга"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VvKlY3SokHGK"
      },
      "outputs": [],
      "source": [
        "# лекции за 2023 год + материалы итмо + книга хопкрофта на русском\n",
        "all_texts = []\n",
        "\n",
        "for i in range(1, 69):\n",
        "    t = extract_text(f\"/content/drive/MyDrive/texts/{i}.pdf\")\n",
        "    # глазами зацепилась за не оч хорошие переносы, поэтому фикс\n",
        "    t = t.replace('-\\n', '').replace('\\n\\n', '\\n')\n",
        "    all_texts.append(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iuzfpEoDHi9"
      },
      "source": [
        "## код"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxCQYjIPsd65"
      },
      "source": [
        "### какие-то мои функции"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rg8_EDpD8yhb"
      },
      "outputs": [],
      "source": [
        "# подсчёт метрики k\n",
        "def my_metrics(user, db, clean_dict=None, trash_dict=None):\n",
        "    user = make_clean(user)\n",
        "    db = make_clean(db)\n",
        "\n",
        "    user = get_words_only(user)\n",
        "    db = get_words_only(db)\n",
        "\n",
        "    if clean_dict is not None:\n",
        "        user = [w for w in user if w in my_dict]\n",
        "        db = [w for w in db if w in my_dict]\n",
        "\n",
        "    if trash_dict is not None:\n",
        "        user = [w for w in user if w not in trash_dict]\n",
        "        db = [w for w in db if w not in trash_dict]\n",
        "\n",
        "    print(user)\n",
        "    print(db)\n",
        "\n",
        "    in_both = list(set(user) & set(db))\n",
        "\n",
        "    return len(in_both) / (len(db) + epsilon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhYu7mWu9wSF"
      },
      "outputs": [],
      "source": [
        "# берём только слова из предложений\n",
        "def get_words_only(sentences_tokenize):\n",
        "    words = [item for sent in sentences_tokenize for item in sent]\n",
        "\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFvD0NeL2Wvu"
      },
      "outputs": [],
      "source": [
        "def make_clean(text):\n",
        "    # чистим текст\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\n|\\t|\\r', ' ', text)\n",
        "    # преобразуем в предложения\n",
        "    sentences = sent_tokenize(text)\n",
        "    # бъём предложения на слова\n",
        "    sentences_tokenize = [tokenizer.tokenize(item) for item in sentences]\n",
        "    # убираем стоп слова\n",
        "    sentences_tokenize = [[item for item in sent if (item not in stopwords_ru and re.match('^[а-я]*$', item))] for sent in sentences_tokenize]\n",
        "    # лемантизируем слова\n",
        "    sentences_tokenize = [[morph.normal_forms(item)[0] for item in sent] for sent in sentences_tokenize]\n",
        "\n",
        "    return sentences_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCoWUMKk30Kf"
      },
      "outputs": [],
      "source": [
        "# словарь популярных слов\n",
        "def get_dict_words(sentences_tokenize):\n",
        "    words = [item for sent in  sentences_tokenize for item in sent]\n",
        "    word_dict = Counter(words)\n",
        "    common = word_dict.most_common()\n",
        "\n",
        "    return common"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtzmzqEu8iQh"
      },
      "outputs": [],
      "source": [
        "# полезные слова = часто встречаем их\n",
        "def get_useful(common):\n",
        "    my_dict = {}\n",
        "\n",
        "    for i, j in common:\n",
        "        if i != 'ε' and len(i) > 2 and re.match('^[а-я]*$', i):\n",
        "            my_dict[i] = j\n",
        "\n",
        "    return my_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLyXh4KBsh7A"
      },
      "source": [
        "### смотрим на текст"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnfIQEDcpTXn"
      },
      "outputs": [],
      "source": [
        "epsilon = 0.001\n",
        "\n",
        "all_sentences = []\n",
        "all_words = []\n",
        "\n",
        "my_dict = {}\n",
        "\n",
        "# мусорные конструкции\n",
        "trash_dict = ['че', 'доказать', 'рассказать', 'ваш', 'привести', 'сделать', 'описать', 'дать', 'что']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntRI3jSxo-Xi"
      },
      "outputs": [],
      "source": [
        "for text in all_texts:\n",
        "    t = make_clean(text)\n",
        "    if len(t) > 0:\n",
        "        all_sentences = all_sentences + t\n",
        "\n",
        "    # частые слова\n",
        "    common = get_dict_words(t)\n",
        "    all_words += [i for (i, j) in common]\n",
        "    # «полезные» слова\n",
        "    useful_dict = get_useful(common)\n",
        "\n",
        "    new_dict = dict(list(my_dict.items()) + list(useful_dict.items()))\n",
        "    my_dict = new_dict.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H7KF6QYK_TE"
      },
      "source": [
        "### предобработка перед doc2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EpWP_U96LFH4"
      },
      "outputs": [],
      "source": [
        "vocab_index = [TaggedDocument(text,[k]) for k, text in enumerate(all_sentences)]\n",
        "\n",
        "model = Doc2Vec(vector_size=300, window=1, min_count=3, workers=6, epochs=20)\n",
        "\n",
        "model.build_vocab(vocab_index)\n",
        "model.train(vocab_index, total_examples=model.corpus_count, epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtVaalz1Mg4i",
        "outputId": "517ce60a-d566-486a-fbfb-cb85af5ff0b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9651259338896611"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "s1 = model.infer_vector('дка'.split())\n",
        "s2 = model.infer_vector('регулярные языки и их свойства'.split())\n",
        "\n",
        "cos_distance = spatial.distance.cosine(s1, s2)\n",
        "\n",
        "cos_distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utLC_GHmN8SS",
        "outputId": "821f3ee5-9c89-4957-b23a-df67bc53f534"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-65-29078a78222d>:1: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
            "  similar_vec = model.docvecs.most_similar([s2], topn=10)\n"
          ]
        }
      ],
      "source": [
        "similar_vec = model.docvecs.most_similar([s2], topn=10)\n",
        "\n",
        "similar_sent = [\" \".join(vocab_index[top[0]].words) for top in similar_vec]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqvEnLZ9spVQ"
      },
      "source": [
        "### предобработка перед w2v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSfuYLAAstDw"
      },
      "outputs": [],
      "source": [
        "bigram = Phrases(all_sentences)\n",
        "bigram_transformer = Phraser(bigram)\n",
        "\n",
        "# генератор текстов с биграммами\n",
        "def bigram_generator():\n",
        "    for text in all_sentences:\n",
        "        yield bigram_transformer[[word for word in text]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLWhw_oRvJUp"
      },
      "outputs": [],
      "source": [
        "trigram = Phrases(bigram_generator())\n",
        "trigram_transformer = Phraser(trigram)\n",
        "\n",
        "def trigram_generator():\n",
        "    for text in all_sentences:\n",
        "        yield trigram_transformer[bigram_transformer[[word for word in text]]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QR-OKoYAPPqO"
      },
      "outputs": [],
      "source": [
        "ok = [i for i in trigram_generator()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fU_fJ8wnvXDt"
      },
      "outputs": [],
      "source": [
        "model = Word2Vec(vector_size=300, window=10, min_count=3, workers=4)\n",
        "model.build_vocab(ok)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moDBxGzkPx8U",
        "outputId": "2757a0cc-6c05-497b-ac3b-80957175ff4f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "21753"
            ]
          },
          "execution_count": 373,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.corpus_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOR_qrPIvdNd",
        "outputId": "a4f6658a-222a-4896-edb8-75e071fc8276"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(56711352, 67266500)"
            ]
          },
          "execution_count": 374,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.train(ok, total_examples=model.corpus_count, epochs=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1Up3Fc31jjO",
        "outputId": "b0e92a6d-da09-4b0c-b743-9184e332dfa8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('нка', 0.47808364033699036),\n",
              " ('автомат', 0.4368232488632202),\n",
              " ('состояние', 0.3628830313682556),\n",
              " ('процесс_допуск', 0.2999037206172943),\n",
              " ('нда', 0.2974991500377655),\n",
              " ('множество', 0.2651441693305969),\n",
              " ('так_быть', 0.2553767263889313),\n",
              " ('эквивалентный_дка', 0.24655848741531372),\n",
              " ('заключительный_состояние', 0.24137888848781586),\n",
              " ('начальный_состояние', 0.2403261363506317)]"
            ]
          },
          "execution_count": 383,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.wv.most_similar('дка')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2QHea_0vgBl"
      },
      "outputs": [],
      "source": [
        "# сохраню потом\n",
        "model.save('wv_model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irA_1fUlviOa"
      },
      "outputs": [],
      "source": [
        "trigram_transformer.save('wv_trigramm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hc3EmqlPrQjB"
      },
      "source": [
        "# подвал"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvs_UWnh2WsV"
      },
      "outputs": [],
      "source": [
        "# чистим текст из пдфки\n",
        "test = make_clean(all_texts[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mZhO3Buy4Sis"
      },
      "outputs": [],
      "source": [
        "# словарь с частыми словами\n",
        "common = get_dict_words(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubMP3X-2-ixl"
      },
      "outputs": [],
      "source": [
        "# словарь с полезными терминами\n",
        "my_dict = get_useful(common)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jW752ahHdmU0"
      },
      "outputs": [],
      "source": [
        "s1 = 'че такое эти ваши производные антимирова'\n",
        "\n",
        "s2 = 'Как можно вычислить частичные производные Антимирова?'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgytyVDsd6LS",
        "outputId": "7f320050-d35a-4d57-e808-2e3489606526"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['такой', 'производный', 'антимиров']\n",
            "['вычислить', 'частичный', 'производный', 'антимиров']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.4998750312421894"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_metrics(s1, s2, trash_dict=trash_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6QZNMrgDqvw"
      },
      "source": [
        "и тут для меня дошло, что, если расписывать вопросы в БЗ, то деление на длину слов вопроса из БЗ (коих будет много) не будет явно и точно отражать близость, но относительно такой штуки можно смотреть динамику"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "eBdqyMMU0_GZ",
        "CxCQYjIPsd65",
        "Hc3EmqlPrQjB"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}