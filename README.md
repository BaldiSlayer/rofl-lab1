# rofl-lab1

## Гайд на временную отправку запросов к LLM

Пока не подняли `LLM` на какой-то виртуалке и обращения к БД пока тоже не сделаны
предлагаю поднимать `LLM` локально. Для этого я сделал `docker-compose.yml`. Поднимать его
очевидно нужно с помощью `docker compose`.

По сути гайд такой:
1) Ставим `Docker` (надеюсь все с этим справятся)
2) Создаем файл `config.py` в папке `LLM/app/utils/Mistral` и не добавляем его в `git`
3) Добавляем туда `api_key = "значение api ключа"`
4) С помощью `docker compose` поднимаем `LLM` командой в духе:
```bash
docker compose -f docker-compose.yml up --build -d
```

3) Отправляем промпты на `API` модельки, можно делать это с помощью [доки](http://localhost:8100/docs
), либо с помощью самописного
скрипта на `Python`, который лежит в `LLM/tester/tester1.py`
4) Пары вопрос ответ можно сохранять в файлике `data/data.json`

P.S. Убиваем контейнер так: =)
```bash
docker compose -f docker-compose.yml down
```